{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d0c1a5-021f-4970-8975-3f452cf19e64",
   "metadata": {},
   "source": [
    "Muhammad Imran Wong bin Muhammad Khalid Wong\n",
    "IS01082642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690da81b-8b73-4d1a-90db-59506ba251a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting capsolver\n",
      "  Downloading capsolver-1.0.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from capsolver) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->capsolver) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->capsolver) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->capsolver) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.26.0->capsolver) (2024.2.2)\n",
      "Downloading capsolver-1.0.7-py3-none-any.whl (9.4 kB)\n",
      "Installing collected packages: capsolver\n",
      "Successfully installed capsolver-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install capsolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09873e63-ea77-4a93-bfb8-5a58d01f9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Collecting reviews on page 1...\n",
      "[INFO] Page 1 complete. Total reviews collected: 5\n",
      "[ACTION] Moving to page 2...\n",
      "\n",
      "[INFO] Collecting reviews on page 2...\n",
      "[INFO] Page 2 complete. Total reviews collected: 10\n",
      "[ACTION] Moving to page 3...\n",
      "\n",
      "[INFO] Collecting reviews on page 3...\n",
      "[INFO] Page 3 complete. Total reviews collected: 15\n",
      "[ACTION] Moving to page 4...\n",
      "\n",
      "[INFO] Collecting reviews on page 4...\n",
      "[INFO] Page 4 complete. Total reviews collected: 20\n",
      "[ACTION] Moving to page 5...\n",
      "\n",
      "[INFO] Collecting reviews on page 5...\n",
      "[INFO] Page 5 complete. Total reviews collected: 25\n",
      "[ACTION] Moving to page 6...\n",
      "\n",
      "[SUCCESS] Scraping finished. 25 reviews saved to 'novencci_sneakers_reviews.csv'.\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import capsolver  # Using CapSolver to bypass CAPTCHA\n",
    "\n",
    "# Provide your CapSolver API key\n",
    "capsolver.api_key = \"YOUR_CAPSOLVER_API_KEY\"\n",
    "\n",
    "# Initialize Chrome with specific options\n",
    "chrome_settings = uc.ChromeOptions()\n",
    "chrome_settings.add_argument(\"--no-sandbox\")\n",
    "chrome_settings.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Launch the browser\n",
    "browser = uc.Chrome(options=chrome_settings)\n",
    "\n",
    "# Product link (Novencci Sneakers)\n",
    "product_url = \"https://www.lazada.com.my/products/novencci-unisex-mens-womens-outdoor-sneakers-sport-i3567542617-s19976126346.html\"\n",
    "\n",
    "# Open product page\n",
    "browser.get(product_url)\n",
    "\n",
    "# Random sleep to mimic human interaction\n",
    "time.sleep(random.uniform(6, 12))\n",
    "\n",
    "# A place to collect review data\n",
    "collected_reviews = []\n",
    "\n",
    "# Set how many pages you want to scrape\n",
    "pages_to_scrape = 5\n",
    "\n",
    "# CAPTCHA handler function\n",
    "def bypass_captcha():\n",
    "    print(\"CAPTCHA found! Trying to bypass with CapSolver...\")\n",
    "    try:\n",
    "        # Attempt to solve CAPTCHA\n",
    "        result = capsolver.solve({\n",
    "            \"type\": \"ReCaptchaV2TaskProxyless\",\n",
    "            \"websiteURL\": browser.current_url,\n",
    "            \"websiteKey\": \"SITE_KEY_HERE\"  # Update with actual site key if known\n",
    "        })\n",
    "        print(f\"CAPTCHA successfully solved: {result}\")\n",
    "        time.sleep(8)  # Give time for CAPTCHA processing\n",
    "        return result\n",
    "    except Exception as error:\n",
    "        print(f\"Failed to bypass CAPTCHA: {error}\")\n",
    "        return None\n",
    "\n",
    "# Start scraping reviews\n",
    "for current_page in range(1, pages_to_scrape + 1):\n",
    "    print(f\"\\n[INFO] Collecting reviews on page {current_page}...\")\n",
    "\n",
    "    # Scroll to bottom to trigger review load\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(random.uniform(5, 9))\n",
    "\n",
    "    # Get reviews on current page\n",
    "    review_items = browser.find_elements(By.XPATH, '//*[@id=\"module_product_review\"]/div/div/div[3]/div[1]/div')\n",
    "\n",
    "    if not review_items:\n",
    "        print(\"[WARNING] No reviews found! Checking for CAPTCHA...\")\n",
    "        captcha_result = bypass_captcha()\n",
    "        if captcha_result:\n",
    "            time.sleep(random.uniform(6, 12))\n",
    "            continue\n",
    "        else:\n",
    "            print(\"[ERROR] Unable to proceed without solving CAPTCHA. Exiting.\")\n",
    "            break\n",
    "\n",
    "    # Extract info from each review\n",
    "    for item in review_items:\n",
    "        try:\n",
    "            user = item.find_element(By.XPATH, \".//div[@class='middle']/span\").text.strip()\n",
    "        except:\n",
    "            user = \"Anonymous\"\n",
    "\n",
    "        try:\n",
    "            review_date = item.find_element(By.XPATH, \".//span[@class='title right']\").text.strip()\n",
    "        except:\n",
    "            review_date = \"No Date Provided\"\n",
    "\n",
    "        try:\n",
    "            review_text = item.find_element(By.XPATH, \".//div[@class='content']\").text.strip()\n",
    "        except:\n",
    "            review_text = \"No Review Text\"\n",
    "\n",
    "        # Append the extracted review data\n",
    "        collected_reviews.append([user, review_date, review_text])\n",
    "\n",
    "    print(f\"[INFO] Page {current_page} complete. Total reviews collected: {len(collected_reviews)}\")\n",
    "\n",
    "    # Navigate to next review page\n",
    "    try:\n",
    "        pagination_block = browser.find_element(By.XPATH, '//*[@id=\"module_product_review\"]/div/div/div[3]/div[2]/div/div')\n",
    "        next_btn = pagination_block.find_element(By.XPATH, f\".//button[contains(@class, 'next-pagination-item') and text()='{current_page + 1}']\")\n",
    "\n",
    "        if next_btn:\n",
    "            print(f\"[ACTION] Moving to page {current_page + 1}...\")\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView();\", next_btn)\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            browser.execute_script(\"window.scrollBy(0, -120);\")\n",
    "            time.sleep(random.uniform(2, 3))\n",
    "\n",
    "            # Click using JS to avoid click interception issues\n",
    "            browser.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            time.sleep(random.uniform(6, 10))\n",
    "\n",
    "            # Wait until page reviews refresh\n",
    "            WebDriverWait(browser, 20).until(EC.staleness_of(review_items[0]))\n",
    "        else:\n",
    "            print(\"[INFO] No further pages detected. Scraping complete.\")\n",
    "            break\n",
    "\n",
    "    except Exception as nav_error:\n",
    "        print(f\"[ERROR] Failed to navigate to next page: {nav_error}\")\n",
    "        break\n",
    "\n",
    "# Close the browser session\n",
    "browser.quit()\n",
    "\n",
    "# Create DataFrame and export reviews to CSV\n",
    "review_df = pd.DataFrame(collected_reviews, columns=[\"User\", \"Date\", \"Review\"])\n",
    "review_df.to_csv(\"novencci_sneakers_reviews.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Scraping finished. {len(collected_reviews)} reviews saved to 'novencci_sneakers_reviews.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8065421-6573-4751-947a-6a03443b6dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
